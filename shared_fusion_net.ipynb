{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mmdet3d.datasets import build_dataset\n",
    "from tools.misc.browse_dataset import build_data_cfg\n",
    "from mmdet3d.models import apply_3d_transformation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter, FixedLocator\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "from mmcv import Config, DictAction\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet3d.ops.voxel.voxelize import voxelization\n",
    "from mmdet3d.ops import DynamicScatter\n",
    "from mmdet3d.ops import (\n",
    "    flat2window_v2,\n",
    "    window2flat_v2,\n",
    "    get_inner_win_inds,\n",
    "    make_continuous_inds,\n",
    "    get_flat2win_inds_v2,\n",
    "    get_window_coors,\n",
    ")\n",
    "from mmdet3d.models.detectors.shared_fusion_net import SharedFusionNet\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "with open(\"/shared-sst/forward_train_input_batch_size_2.pkl\", \"rb\") as f:\n",
    "    forward_train_input = pickle.load(f)\n",
    "\n",
    "points, img, img_metas, gt_bboxes_3d, gt_labels_3d = forward_train_input.values()\n",
    "img = img.to(device).float()\n",
    "points = [p.float() for p in points]\n",
    "\n",
    "cfg = Config.fromfile(\"configs/shared_sst/shared_fusion_lidar_detection_debug_config.py\")\n",
    "model = build_model(cfg.model, train_cfg=cfg.get(\"train_cfg\"), test_cfg=cfg.get(\"test_cfg\"))\n",
    "#model.init_weights()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_coors(unflattened_patches, patch_size):\n",
    "    device = unflattened_patches.device\n",
    "    batch_size, height, width = unflattened_patches.shape[0], unflattened_patches.shape[1], unflattened_patches.shape[2]\n",
    "    patch_coors = torch.zeros((height * width * batch_size, 4), device=device)\n",
    "    \n",
    "    # Width indices\n",
    "    patch_coors[:, 3] = torch.arange(width).repeat(height * batch_size)\n",
    "\n",
    "    # Height and batch indices\n",
    "    height_indices = np.repeat(np.arange(height), width)\n",
    "    for batch_index in range(batch_size):\n",
    "        patch_coors[batch_index * height * width : (batch_index + 1) * height * width, 0] = batch_index\n",
    "        patch_coors[batch_index * height * width : (batch_index + 1) * height * width, 2] = torch.from_numpy(height_indices)\n",
    "\n",
    "    # Scale to image size\n",
    "    patch_coors[:, 2] = patch_coors[:, 2] * patch_size + patch_size // 2\n",
    "    patch_coors[:, 3] = patch_coors[:, 3] * patch_size + patch_size // 2\n",
    "    return patch_coors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxelize point cloud\n",
    "voxels, coors = model.voxelize(points)  # [Batch, Z, Y, X]\n",
    "batch_size = coors[-1, 0].item() + 1\n",
    "voxel_features, voxel_feature_coors = model.voxel_encoder(voxels, coors)\n",
    "voxel_mean, _ = model.voxel_encoder.cluster_scatter(voxels, coors)\n",
    "\n",
    "# Patchify wide image\n",
    "img_wide = torch.cat([img[:, i] for i in model.middle_encoder.camera_order], dim=3)\n",
    "patches = model.patch_embedder(img_wide)\n",
    "\n",
    "# Convert patches to same format as voxels\n",
    "unflattened_patches = patches[0].unflatten(1, patches[1])\n",
    "patch_features = patches[0].flatten(0, 1)\n",
    "patch_coors = get_patch_coors(unflattened_patches, model.patch_embedder.projection.kernel_size[0])\n",
    "\n",
    "\n",
    "sst_info = model.middle_encoder(\n",
    "    voxel_features,\n",
    "    voxel_feature_coors,\n",
    "    voxel_mean,\n",
    "    patch_features,\n",
    "    patch_coors,\n",
    "    img_metas,\n",
    "    batch_size,\n",
    ")\n",
    "\n",
    "[batch_canvas] = model.backbone(sst_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/mmdet/core/anchor/builder.py:16: UserWarning: ``build_anchor_generator`` would be deprecated soon, please use ``build_prior_generator`` \n",
      "  '``build_anchor_generator`` would be deprecated soon, please use '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SharedFusionNet(\n",
       "  (backbone): SharedSST(\n",
       "    (block_list): ModuleList(\n",
       "      (0): BasicShiftBlockV2(\n",
       "        (encoder_list): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (win_attn): WindowAttention(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (linear1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "            (linear2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0, inplace=False)\n",
       "            (dropout2): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (win_attn): WindowAttention(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (linear1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "            (linear2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0, inplace=False)\n",
       "            (dropout2): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): BasicShiftBlockV2(\n",
       "        (encoder_list): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (win_attn): WindowAttention(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (linear1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "            (linear2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0, inplace=False)\n",
       "            (dropout2): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (win_attn): WindowAttention(\n",
       "              (self_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (linear1): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (dropout): Dropout(p=0, inplace=False)\n",
       "            (linear2): Linear(in_features=8, out_features=8, bias=True)\n",
       "            (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0, inplace=False)\n",
       "            (dropout2): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_layer): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): NaiveSyncBatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): NaiveSyncBatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (1): NaiveSyncBatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): SECONDFPN(\n",
       "    (deblocks): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): NaiveSyncBatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  init_cfg=[{'type': 'Kaiming', 'layer': 'ConvTranspose2d'}, {'type': 'Constant', 'layer': 'NaiveSyncBatchNorm2d', 'val': 1.0}]\n",
       "  (bbox_head): Anchor3DHead(\n",
       "    (loss_cls): FocalLoss()\n",
       "    (loss_bbox): SmoothL1Loss()\n",
       "    (loss_dir): CrossEntropyLoss()\n",
       "    (conv_cls): Conv2d(384, 140, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_reg): Conv2d(384, 126, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_dir_cls): Conv2d(384, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  init_cfg={'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01, 'override': {'type': 'Normal', 'name': 'conv_cls', 'std': 0.01, 'bias_prob': 0.01}}\n",
       "  (voxel_layer): Voxelization(voxel_size=(0.5, 0.5, 8), point_cloud_range=[-50, -50, -5, 50, 50, 3], max_num_points=-1, max_voxels=(-1, -1))\n",
       "  (voxel_encoder): DynamicVFE(\n",
       "    (scatter): DynamicScatter(voxel_size=(0.5, 0.5, 8), point_cloud_range=[-50, -50, -5, 50, 50, 3], average_points=True)\n",
       "    (vfe_layers): ModuleList(\n",
       "      (0): DynamicVFELayer(\n",
       "        (norm): NaiveSyncBatchNorm1d(4, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (linear): Linear(in_features=10, out_features=4, bias=False)\n",
       "      )\n",
       "      (1): DynamicVFELayer(\n",
       "        (norm): NaiveSyncBatchNorm1d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        (linear): Linear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (vfe_scatter): DynamicScatter(voxel_size=(0.5, 0.5, 8), point_cloud_range=[-50, -50, -5, 50, 50, 3], average_points=False)\n",
       "    (cluster_scatter): DynamicScatter(voxel_size=(0.5, 0.5, 8), point_cloud_range=[-50, -50, -5, 50, 50, 3], average_points=True)\n",
       "  )\n",
       "  (middle_encoder): SharedSSTInputLayer()\n",
       "  (patch_embedder): PatchEmbed(\n",
       "    (adap_padding): AdaptivePadding()\n",
       "    (projection): Conv2d(3, 8, kernel_size=(64, 64), stride=(64, 64))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from mmdet3d.datasets import build_dataset\n",
    "from tools.misc.browse_dataset import build_data_cfg\n",
    "from mmdet3d.models import apply_3d_transformation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter, FixedLocator\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "from mmcv import Config, DictAction\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet3d.ops.voxel.voxelize import voxelization\n",
    "from mmdet3d.ops import DynamicScatter\n",
    "from mmdet3d.ops import (\n",
    "    flat2window_v2,\n",
    "    window2flat_v2,\n",
    "    get_inner_win_inds,\n",
    "    make_continuous_inds,\n",
    "    get_flat2win_inds_v2,\n",
    "    get_window_coors,\n",
    ")\n",
    "from mmdet3d.models.detectors.shared_fusion_net import SharedFusionNet\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "with open(\"/shared-sst/forward_train_input_batch_size_2.pkl\", \"rb\") as f:\n",
    "    forward_train_input = pickle.load(f)\n",
    "\n",
    "points, img, img_metas, gt_bboxes_3d, gt_labels_3d = forward_train_input.values()\n",
    "img = img.to(device).float()\n",
    "points = [p.float() for p in points]\n",
    "\n",
    "cfg = Config.fromfile(\"configs/shared_sst/shared_fusion_lidar_detection_debug_config.py\")\n",
    "model = build_model(cfg.model, train_cfg=cfg.get(\"train_cfg\"), test_cfg=cfg.get(\"test_cfg\"))\n",
    "#model.init_weights()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_coors(unflattened_patches, patch_size):\n",
    "    device = unflattened_patches.device\n",
    "    batch_size, height, width = unflattened_patches.shape[0], unflattened_patches.shape[1], unflattened_patches.shape[2]\n",
    "    patch_coors = torch.zeros((height * width * batch_size, 4), device=device)\n",
    "    \n",
    "    # Width indices\n",
    "    patch_coors[:, 3] = torch.arange(width).repeat(height * batch_size)\n",
    "\n",
    "    # Height and batch indices\n",
    "    height_indices = np.repeat(np.arange(height), width)\n",
    "    for batch_index in range(batch_size):\n",
    "        patch_coors[batch_index * height * width : (batch_index + 1) * height * width, 0] = batch_index\n",
    "        patch_coors[batch_index * height * width : (batch_index + 1) * height * width, 2] = torch.from_numpy(height_indices)\n",
    "\n",
    "    # Scale to image size\n",
    "    patch_coors[:, 2] = patch_coors[:, 2] * patch_size + patch_size // 2\n",
    "    patch_coors[:, 3] = patch_coors[:, 3] * patch_size + patch_size // 2\n",
    "    return patch_coors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop_info is set to {0: {'max_tokens': 30, 'drop_range': (0, 30)}, 1: {'max_tokens': 60, 'drop_range': (30, 60)}, 2: {'max_tokens': 100, 'drop_range': (60, 100)}, 3: {'max_tokens': 200, 'drop_range': (100, 200)}, 4: {'max_tokens': 250, 'drop_range': (200, 100000)}}, in input_layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "# Voxelize point cloud\n",
    "voxels, coors = model.voxelize(points)  # [Batch, Z, Y, X]\n",
    "batch_size = coors[-1, 0].item() + 1\n",
    "voxel_features, voxel_feature_coors = model.voxel_encoder(voxels, coors)\n",
    "voxel_mean, _ = model.voxel_encoder.cluster_scatter(voxels, coors)\n",
    "\n",
    "# Patchify wide image\n",
    "img_wide = torch.cat([img[:, i] for i in model.middle_encoder.camera_order], dim=3)\n",
    "patches = model.patch_embedder(img_wide)\n",
    "\n",
    "# Convert patches to same format as voxels\n",
    "unflattened_patches = patches[0].unflatten(1, patches[1])\n",
    "patch_features = patches[0].flatten(0, 1)\n",
    "patch_coors = get_patch_coors(unflattened_patches, model.patch_embedder.projection.kernel_size[0])\n",
    "\n",
    "\n",
    "sst_info = model.middle_encoder(\n",
    "    voxel_features,\n",
    "    voxel_feature_coors,\n",
    "    voxel_mean,\n",
    "    patch_features,\n",
    "    patch_coors,\n",
    "    img_metas,\n",
    "    batch_size,\n",
    ")\n",
    "\n",
    "[batch_canvas] = model.backbone(sst_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
